# rag.py
# Complete Retrieval Augmented Generation (RAG) Implementation
# This is a FULLY FUNCTIONAL RAG system that integrates all components:
# - Text chunking for document preprocessing
# - Embeddings for semantic representation
# - Vector database for efficient storage and retrieval
# - LLM integration for response generation

import sys
import os
import numpy as np
from typing import List, Dict, Any, Optional

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))

# Import our custom modules
try:
    # Text processing
    from module06_text_processing.chunking import chunk_text, chunk_document
    # Embeddings
    from module03_embeddings_search.embeddings import get_embeddings
    # Vector database
    from module04_vector_databases.vector_databases import SimpleVectorDB
    # LLM integration
    from module07_llm_prompting.llm import generate_text
    
    FULL_IMPLEMENTATION = True
    print("âœ… All modules imported successfully - Full RAG implementation available!")

except ImportError as e:
    print(f"âš ï¸  Some modules not available: {e}")
    print("ğŸ”„ Using fallback implementations...")
    FULL_IMPLEMENTATION = False

class RAGSystem:
    """
    Complete Retrieval Augmented Generation System

    This class implements a full RAG pipeline:
    1. Document ingestion and chunking
    2. Embedding generation
    3. Vector storage and indexing
    4. Retrieval based on semantic similarity
    5. Response generation using retrieved context
    """

    def __init__(self):
        """Initialize the RAG system"""
        self.vector_db = SimpleVectorDB()
        self.documents = []
        self.chunks = []
        print("ğŸ—ï¸  RAG System initialized")

    def add_documents(self, documents: List[str], chunk_size: int = 500, overlap: int = 50):
        """
        Add documents to the RAG system

        Args:
            documents: List of text documents
            chunk_size: Size of each text chunk
            overlap: Overlap between chunks
        """
        print(f"ğŸ“„ Processing {len(documents)} documents...")

        self.documents = documents
        all_chunks = []

        # Chunk all documents
        for doc_idx, doc in enumerate(documents):
            chunks = chunk_text(doc, chunk_size, overlap)
            for chunk_idx, chunk in enumerate(chunks):
                all_chunks.append({
                    'text': chunk,
                    'doc_id': doc_idx,
                    'chunk_id': chunk_idx
                })

        self.chunks = all_chunks
        print(f"âœ‚ï¸  Created {len(all_chunks)} text chunks")

        # Generate embeddings for all chunks
        print("ğŸ” Generating embeddings...")
        chunk_texts = [chunk['text'] for chunk in all_chunks]
        embeddings = get_embeddings(chunk_texts)

        # Store embedding dimension for consistency
        self.embedding_dim = embeddings.shape[1] if len(embeddings.shape) > 1 else len(embeddings[0])
        print(f"ğŸ“ Embedding dimension: {self.embedding_dim}")

        # Store in vector database
        print("ğŸ’¾ Storing in vector database...")
        for i, chunk in enumerate(all_chunks):
            self.vector_db.add_vector(
                embeddings[i].tolist() if len(embeddings.shape) > 1 else embeddings[i],
                {
                    'text': chunk['text'],
                    'doc_id': chunk['doc_id'],
                    'chunk_id': chunk['chunk_id']
                }
            )

        print("âœ… Documents added successfully!")

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        Retrieve relevant documents for a query

        Args:
            query: Search query
            top_k: Number of top results to return

        Returns:
            List of retrieved documents with metadata
        """
        # Generate embedding for query
        query_embedding = get_embeddings([query])[0]

        # Ensure consistent dimensions
        if hasattr(self, 'embedding_dim'):
            if len(query_embedding) != self.embedding_dim:
                # Pad or truncate to match stored embeddings
                if len(query_embedding) < self.embedding_dim:
                    query_embedding = np.pad(query_embedding, (0, self.embedding_dim - len(query_embedding)))
                else:
                    query_embedding = query_embedding[:self.embedding_dim]

        # Search vector database
        results = self.vector_db.search(query_embedding.tolist(), top_k)

        return results

    def generate_response(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> str:
        """
        Generate response using retrieved documents

        Args:
            query: Original query
            retrieved_docs: Retrieved relevant documents

        Returns:
            Generated response
        """
        if not retrieved_docs:
            return "I couldn't find relevant information to answer your question."

        # Combine retrieved context
        context_parts = []
        for i, doc in enumerate(retrieved_docs):
            context_parts.append(f"[Document {i+1}]: {doc['metadata']['text']}")

        context = "\n\n".join(context_parts)

        # Create prompt for LLM
        prompt = f"""
Based on the following retrieved information, please answer the question accurately and comprehensively.

QUESTION: {query}

RETRIEVED INFORMATION:
{context}

Please provide a detailed answer based on the retrieved information above. If the information doesn't fully answer the question, acknowledge the limitations.
"""

        # Generate response using LLM
        response = generate_text(prompt)

        return response

    def query(self, query: str, top_k: int = 3) -> Dict[str, Any]:
        """
        Complete RAG query pipeline

        Args:
            query: User query
            top_k: Number of documents to retrieve

        Returns:
            Dictionary with query, retrieved docs, and generated response
        """
        print(f"ğŸ” Processing query: '{query}'")

        # Step 1: Retrieve relevant documents
        retrieved_docs = self.retrieve(query, top_k)
        print(f"ğŸ“‹ Retrieved {len(retrieved_docs)} relevant documents")

        # Step 2: Generate response
        response = self.generate_response(query, retrieved_docs)
        print("ğŸ¤– Generated response")

        return {
            'query': query,
            'retrieved_documents': retrieved_docs,
            'response': response,
            'num_docs_retrieved': len(retrieved_docs)
        }


def main():
    """Demonstrate the complete RAG system"""
    print("ğŸš€ Complete RAG System Demonstration")
    print("=" * 50)

    # Initialize RAG system
    rag = RAGSystem()

    # Sample documents about RAG and AI
    documents = [
        """
        Retrieval Augmented Generation (RAG) is a technique that combines the power of retrieval systems
        with generative AI to provide more accurate and contextually relevant responses. RAG works by
        first retrieving relevant information from a knowledge base, then using that information to
        guide the generation of responses by a large language model.
        """,

        """
        Vector databases are specialized storage systems designed to efficiently store and query
        high-dimensional vectors. They are essential for RAG applications because they enable fast
        similarity search across large collections of embeddings. Popular vector databases include
        Pinecone, Weaviate, and Chroma.
        """,

        """
        Text embeddings are dense vector representations that capture the semantic meaning of text.
        They transform textual data into numerical vectors that can be processed by machine learning
        algorithms. Similar meanings are represented by similar vectors, enabling mathematical
        operations on semantic concepts.
        """,

        """
        Large Language Models (LLMs) like GPT are powerful but have limitations including outdated
        knowledge and potential for hallucinations. RAG addresses these limitations by grounding
        responses in external, verifiable knowledge sources, making the outputs more accurate and
        trustworthy.
        """,

        """
        The RAG pipeline consists of several key steps: document ingestion and preprocessing,
        text chunking, embedding generation, vector storage, retrieval based on similarity search,
        and finally response generation using the retrieved context. Each step is crucial for
        building an effective RAG system.
        """
    ]

    # Add documents to the system
    rag.add_documents(documents)

    print("\n" + "=" * 50)
    print("ğŸ§ª Testing RAG Queries")
    print("=" * 50)

    # Test queries
    test_queries = [
        "What is Retrieval Augmented Generation?",
        "How do vector databases work?",
        "What are the limitations of large language models?",
        "How does the RAG pipeline work?"
    ]

    for query in test_queries:
        print(f"\nâ“ Query: {query}")
        result = rag.query(query)

        print(f"ğŸ“„ Retrieved {result['num_docs_retrieved']} documents")
        print(f"ğŸ¤– Response: {result['response']}")
        print("-" * 50)

    print("\nğŸ‰ RAG System demonstration complete!")
    print("ğŸ’¡ The system successfully combines retrieval and generation!")


if __name__ == "__main__":
    main()