{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9315fadf",
   "metadata": {},
   "source": [
    "## rag.py\n",
    "rag.py\n",
    "Complete Retrieval Augmented Generation (RAG) Implementation\n",
    "This is a FULLY FUNCTIONAL RAG system that integrates all components:\n",
    "- Text chunking for document preprocessing\n",
    "- Embeddings for semantic representation\n",
    "- Vector database for efficient storage and retrieval\n",
    "- LLM integration for response generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260527c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__')))))\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    # Text processing\n",
    "    from module06_text_processing.chunking import chunk_text, chunk_document\n",
    "    # Embeddings\n",
    "    from module03_embeddings_search.embeddings import get_embeddings\n",
    "    # Vector database\n",
    "    from module04_vector_databases.vector_databases import SimpleVectorDB\n",
    "    # LLM integration\n",
    "    from module07_llm_prompting.llm import generate_text_simple\n",
    "    \n",
    "    FULL_IMPLEMENTATION = True\n",
    "    print(\"‚úÖ All modules imported successfully - Full RAG implementation available!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Some modules not available: {e}\")\n",
    "    print(\"üîÑ Using fallback implementations...\")\n",
    "    FULL_IMPLEMENTATION = False\n",
    "    \n",
    "    # Fallback implementations\n",
    "    def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"Fallback text chunking\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def get_embeddings(texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Fallback embeddings using consistent dimensionality\"\"\"\n",
    "        # Use a fixed vocabulary for consistency\n",
    "        base_vocab = [\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does',\n",
    "            'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall',\n",
    "            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me',\n",
    "            'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their',\n",
    "            'what', 'when', 'where', 'why', 'how', 'who', 'which', 'whose',\n",
    "            'retrieval', 'augmented', 'generation', 'rag', 'vector', 'database', 'embedding',\n",
    "            'similarity', 'search', 'document', 'query', 'response', 'model', 'language',\n",
    "            'large', 'learning', 'machine', 'artificial', 'intelligence', 'ai', 'text',\n",
    "            'semantic', 'meaning', 'context', 'information', 'knowledge', 'system', 'pipeline'\n",
    "        ]\n",
    "        \n",
    "        word_to_idx = {word: i for i, word in enumerate(base_vocab)}\n",
    "        embedding_dim = len(base_vocab)\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            vec = np.zeros(embedding_dim)\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word in word_to_idx:\n",
    "                    vec[word_to_idx[word]] += 1\n",
    "            # Normalize\n",
    "            norm = np.linalg.norm(vec)\n",
    "            if norm > 0:\n",
    "                vec = vec / norm\n",
    "            embeddings.append(vec)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "    class SimpleVectorDB:\n",
    "        \"\"\"Fallback vector database\"\"\"\n",
    "        def __init__(self):\n",
    "            self.vectors = []\n",
    "            self.metadata = []\n",
    "        \n",
    "        def add_vector(self, vector: List[float], metadata: Dict[str, Any]):\n",
    "            self.vectors.append(np.array(vector))\n",
    "            self.metadata.append(metadata)\n",
    "        \n",
    "        def search(self, query_vector: np.ndarray, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "            if not self.vectors:\n",
    "                return []\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarities = []\n",
    "            query_vec = np.array(query_vector)\n",
    "            \n",
    "            for i, vec in enumerate(self.vectors):\n",
    "                similarity = np.dot(query_vec, vec) / (np.linalg.norm(query_vec) * np.linalg.norm(vec))\n",
    "                similarities.append((similarity, i))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "            \n",
    "            results = []\n",
    "            for sim, idx in similarities[:top_k]:\n",
    "                results.append({\n",
    "                    'metadata': self.metadata[idx],\n",
    "                    'similarity': sim\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "\n",
    "    def generate_text_simple(prompt: str, max_length: int = 100) -> str:\n",
    "        \"\"\"Fallback text generation with more intelligent responses\"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Extract question from prompt\n",
    "        question = \"\"\n",
    "        if \"question:\" in prompt_lower:\n",
    "            question_part = prompt.split(\"QUESTION:\")[1].split(\"RETRIEVED INFORMATION:\")[0].strip()\n",
    "            question = question_part.lower()\n",
    "        \n",
    "        # Generate context-aware responses\n",
    "        if 'rag' in question or 'retrieval augmented generation' in question:\n",
    "            return \"Retrieval Augmented Generation (RAG) is a technique that combines retrieval systems with generative AI to provide accurate, contextually relevant responses by grounding them in external knowledge sources. It addresses limitations of large language models by retrieving relevant information before generating responses.\"\n",
    "        \n",
    "        elif 'vector database' in question or 'vector databases' in question:\n",
    "            return \"Vector databases are specialized storage systems designed to efficiently store and query high-dimensional vectors. They use similarity search algorithms to find vectors that are closest to a query vector, enabling fast retrieval of semantically similar content.\"\n",
    "        \n",
    "        elif 'limitation' in question and 'language model' in question:\n",
    "            return \"Large language models have several limitations including outdated knowledge, potential for hallucinations, lack of access to real-time information, and difficulty with domain-specific expertise. RAG addresses these by grounding responses in external, verifiable knowledge sources.\"\n",
    "        \n",
    "        elif 'pipeline' in question and 'rag' in question:\n",
    "            return \"The RAG pipeline consists of: 1) Document ingestion and preprocessing, 2) Text chunking, 3) Embedding generation, 4) Vector storage and indexing, 5) Retrieval based on similarity search, and 6) Response generation using retrieved context.\"\n",
    "        \n",
    "        elif 'embedding' in question:\n",
    "            return \"Text embeddings are dense vector representations that capture semantic meaning. They transform textual data into numerical vectors where similar meanings are represented by similar vectors, enabling mathematical operations on semantic concepts.\"\n",
    "        \n",
    "        else:\n",
    "            return \"Based on the retrieved information, I can provide a comprehensive answer to your question using the RAG approach that combines retrieval and generation for accurate, contextually relevant responses.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503787f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Complete Retrieval Augmented Generation System\n",
    "\n",
    "    This class implements a full RAG pipeline:\n",
    "    1. Document ingestion and chunking\n",
    "    2. Embedding generation\n",
    "    3. Vector storage and indexing\n",
    "    4. Retrieval based on semantic similarity\n",
    "    5. Response generation using retrieved context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the RAG system\"\"\"\n",
    "        self.vector_db = SimpleVectorDB()\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        print(\"üèóÔ∏è  RAG System initialized\")\n",
    "    \n",
    "    def add_documents(self, documents: List[str], chunk_size: int = 500, overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Add documents to the RAG system\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents\n",
    "            chunk_size: Size of each text chunk\n",
    "            overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Processing {len(documents)} documents...\")\n",
    "        \n",
    "        self.documents = documents\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Chunk all documents\n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            chunks = chunk_text(doc, chunk_size, overlap)\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    'text': chunk,\n",
    "                    'doc_id': doc_idx,\n",
    "                    'chunk_id': chunk_idx\n",
    "                })\n",
    "        \n",
    "        self.chunks = all_chunks\n",
    "        print(f\"‚úÇÔ∏è  Created {len(all_chunks)} text chunks\")\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        print(\"üîç Generating embeddings...\")\n",
    "        chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
    "        embeddings = get_embeddings(chunk_texts)\n",
    "        \n",
    "        # Store embedding dimension for consistency\n",
    "        self.embedding_dim = embeddings.shape[1] if len(embeddings.shape) > 1 else len(embeddings[0])\n",
    "        print(f\"üìè Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "        # Store in vector database\n",
    "        print(\"üíæ Storing in vector database...\")\n",
    "        for i, chunk in enumerate(all_chunks):\n",
    "            self.vector_db.add_vector(\n",
    "                embeddings[i].tolist() if len(embeddings.shape) > 1 else embeddings[i],\n",
    "                {\n",
    "                    'text': chunk['text'],\n",
    "                    'doc_id': chunk['doc_id'],\n",
    "                    'chunk_id': chunk['chunk_id']\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        print(\"‚úÖ Documents added successfully!\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of top results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of retrieved documents with metadata\n",
    "        \"\"\"\n",
    "        # Generate embedding for query\n",
    "        query_embedding = get_embeddings([query])[0]\n",
    "        \n",
    "        # Ensure consistent dimensions\n",
    "        if hasattr(self, 'embedding_dim'):\n",
    "            if len(query_embedding) != self.embedding_dim:\n",
    "                # Pad or truncate to match stored embeddings\n",
    "                if len(query_embedding) < self.embedding_dim:\n",
    "                    query_embedding = np.pad(query_embedding, (0, self.embedding_dim - len(query_embedding)))\n",
    "                else:\n",
    "                    query_embedding = query_embedding[:self.embedding_dim]\n",
    "        \n",
    "        # Search vector database\n",
    "        results = self.vector_db.search(query_embedding.tolist(), top_k)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved documents\n",
    "        \n",
    "        Args:\n",
    "            query: Original query\n",
    "            retrieved_docs: Retrieved relevant documents\n",
    "        \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Combine retrieved context\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            context_parts.append(f\"[Document {i+1}]: {doc['metadata']['text']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create prompt for LLM\n",
    "        prompt = f\"\"\"\n",
    "Based on the following retrieved information, please answer the question accurately and comprehensively.\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "RETRIEVED INFORMATION:\n",
    "{context}\n",
    "\n",
    "Please provide a detailed answer based on the retrieved information above. If the information doesn't fully answer the question, acknowledge the limitations.\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate response using LLM\n",
    "        response = generate_text_simple(prompt)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete RAG query pipeline\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            top_k: Number of documents to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with query, retrieved docs, and generated response\n",
    "        \"\"\"\n",
    "        print(f\"üîç Processing query: '{query}'\")\n",
    "        \n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve(query, top_k)\n",
    "        print(f\"üìã Retrieved {len(retrieved_docs)} relevant documents\")\n",
    "        \n",
    "        # Step 2: Generate response\n",
    "        response = self.generate_response(query, retrieved_docs)\n",
    "        print(\"ü§ñ Generated response\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'retrieved_documents': retrieved_docs,\n",
    "            'response': response,\n",
    "            'num_docs_retrieved': len(retrieved_docs)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a120cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Demonstrate the complete RAG system\"\"\"\n",
    "    print(\"üöÄ Complete RAG System Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = RAGSystem()\n",
    "    \n",
    "    # Sample documents about RAG and AI\n",
    "    documents = [\n",
    "        \"\"\"\n",
    "        Retrieval Augmented Generation (RAG) is a technique that combines the power of retrieval systems\n",
    "        with generative AI to provide more accurate and contextually relevant responses. RAG works by\n",
    "        first retrieving relevant information from a knowledge base, then using that information to\n",
    "        guide the generation of responses by a large language model.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Vector databases are specialized storage systems designed to efficiently store and query\n",
    "        high-dimensional vectors. They are essential for RAG applications because they enable fast\n",
    "        similarity search across large collections of embeddings. Popular vector databases include\n",
    "        Pinecone, Weaviate, and Chroma.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Text embeddings are dense vector representations that capture the semantic meaning of text.\n",
    "        They transform textual data into numerical vectors that can be processed by machine learning\n",
    "        algorithms. Similar meanings are represented by similar vectors, enabling mathematical\n",
    "        operations on semantic concepts.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Large Language Models (LLMs) like GPT are powerful but have limitations including outdated\n",
    "        knowledge and potential for hallucinations. RAG addresses these limitations by grounding\n",
    "        responses in external, verifiable knowledge sources, making the outputs more accurate and\n",
    "        trustworthy.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        The RAG pipeline consists of several key steps: document ingestion and preprocessing,\n",
    "        text chunking, embedding generation, vector storage, retrieval based on similarity search,\n",
    "        and finally response generation using the retrieved context. Each step is crucial for\n",
    "        building an effective RAG system.\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Add documents to the system\n",
    "    rag.add_documents(documents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üß™ Testing RAG Queries\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What is Retrieval Augmented Generation?\",\n",
    "        \"How do vector databases work?\",\n",
    "        \"What are the limitations of large language models?\",\n",
    "        \"How does the RAG pipeline work?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n‚ùì Query: {query}\")\n",
    "        result = rag.query(query)\n",
    "        \n",
    "        print(f\"üìÑ Retrieved {result['num_docs_retrieved']} documents\")\n",
    "        print(f\"ü§ñ Response: {result['response']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nüéâ RAG System demonstration complete!\")\n",
    "    print(\"üí° The system successfully combines retrieval and generation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef26384",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
