{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module07_llm_prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine_tuning.py\n",
    "fine_tuning.py\n",
    "Explanation of Fine-Tuning (FT-transform)\n",
    "Fine-tuning is the process of taking a pre-trained model and training it further on a specific dataset.\n",
    "This adapts the model to perform better on domain-specific tasks.\n",
    "Fine-tuning leverages transfer learning to adapt general knowledge to specific domains.\n",
    "Simple example: Fine-tuning a simple classifier\n",
    "Note: This is a conceptual example; real fine-tuning requires large datasets and compute\n",
    "Import required libraries for the fine-tuning demonstration\n",
    "numpy for numerical operations and data generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# LogisticRegression as a simple model to demonstrate fine-tuning concept\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# train_test_split for dividing data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# accuracy_score for evaluating model performance\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that simulates loading a pre-trained model\n",
    "# In reality, this would load a model like BERT or GPT that was pre-trained on massive datasets\n",
    "def pre_trained_model():\n",
    "    # Return a LogisticRegression with random initialization (simulating pre-trained weights)\n",
    "    return LogisticRegression(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning function that adapts the pre-trained model to new data\n",
    "def fine_tune_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fine-tune the model on new data.\n",
    "    This simulates the process of continuing training on domain-specific data.\n",
    "    In real fine-tuning, you'd use a smaller learning rate to avoid catastrophic forgetting.\n",
    "    \"\"\"\n",
    "    # Fit the model to the new training data\n",
    "    # This adjusts the model's parameters to better fit the specific task/domain\n",
    "    model.fit(X_train, y_train)\n",
    "    # Return the fine-tuned model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with dummy data to demonstrate the fine-tuning process\n",
    "# Generate synthetic data for demonstration\n",
    "np.random.seed(42)  # For reproducible results\n",
    "X = np.random.rand(100, 5)  # 100 samples with 5 features each\n",
    "y = np.random.randint(0, 2, 100)  # Binary classification labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model (simulated)\n",
    "model = pre_trained_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model on the training data\n",
    "# This adapts the model from its general pre-training to the specific task\n",
    "fine_tuned_model = fine_tune_model(model, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on the test set\n",
    "y_pred = fine_tuned_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Fine-tuned model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world fine-tuning process for LLMs involves several key steps:\n",
    "1. Loading a pre-trained model (e.g., BERT, GPT) that was trained on massive general datasets\n",
    "2. Preparing domain-specific dataset with examples relevant to your use case\n",
    "3. Training on the new data with smaller learning rate to preserve general knowledge\n",
    "4. Evaluating performance on validation set to ensure the model improved for your domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm.py\n",
    "llm.py\n",
    "Explanation of Large Language Model (LLM)\n",
    "LLMs are transformer-based models trained on large datasets to understand and generate human-like text.\n",
    "They use attention mechanisms to process sequences of text.\n",
    "LLMs can perform various tasks like text generation, translation, summarization, and question answering.\n",
    "Simple example: Using a pre-trained LLM for text generation\n",
    "Note: This requires installing transformers library: pip install transformers\n",
    "Try to import and use the transformers library for real LLM interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Import the pipeline utility from transformers for easy model usage\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Load a simple pre-trained model for text generation\n",
    "    # GPT-2 is a popular open-source LLM for text generation tasks\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "    # Function to generate text based on a prompt\n",
    "    def generate_text(prompt, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate text based on a prompt using GPT-2.\n",
    "        This function takes a text prompt and extends it using the language model.\n",
    "        \"\"\"\n",
    "        # Use the pipeline to generate text\n",
    "        # max_length limits the total output length\n",
    "        # num_return_sequences=1 means return one generated text\n",
    "        result = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "        # Extract the generated text from the result\n",
    "        return result[0]['generated_text']\n",
    "\n",
    "    # Example usage of the text generation function\n",
    "    prompt = \"The future of AI is\"\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Generated text: {generated}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Transformers library not installed. Install with: pip install transformers\")\n",
    "    # Fallback: Simple rule-based text generation for demonstration\n",
    "    def simple_generate(prompt):\n",
    "        # Basic pattern matching for simple text extension\n",
    "        if \"AI\" in prompt:\n",
    "            return prompt + \" exciting and full of possibilities.\"\n",
    "        else:\n",
    "            return prompt + \" something interesting.\"\n",
    "\n",
    "    # Demonstrate the fallback generation\n",
    "    print(simple_generate(\"The future of\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt_engineering.py\n",
    "prompt_engineering.py\n",
    "Explanation of Prompt Engineering\n",
    "Prompt engineering is the practice of crafting effective prompts to get desired outputs from LLMs.\n",
    "It involves structuring questions, providing context, and using techniques like few-shot learning.\n",
    "Good prompts can dramatically improve LLM performance and consistency.\n",
    "Simple examples of different prompting techniques\n",
    "Zero-shot prompting function - ask model to perform task without examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompting(llm_function, task_description, input_text):\n",
    "    \"\"\"\n",
    "    Zero-shot prompting: Ask the model to perform a task without examples.\n",
    "    This relies on the model's pre-trained knowledge and general capabilities.\n",
    "    Works well for tasks the model has seen during training.\n",
    "    \"\"\"\n",
    "    # Construct prompt with clear task description and input\n",
    "    prompt = f\"{task_description}\\n\\nInput: {input_text}\\nOutput:\"\n",
    "    # Call the LLM function with the constructed prompt\n",
    "    return llm_function(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting function - provide examples before the actual task\n",
    "def few_shot_prompting(llm_function, examples, task_description, input_text):\n",
    "    \"\"\"\n",
    "    Few-shot prompting: Provide examples before asking for the task.\n",
    "    This helps the model understand the desired format and reasoning pattern.\n",
    "    Particularly effective for tasks requiring specific output formats.\n",
    "    \"\"\"\n",
    "    # Start with the task description\n",
    "    prompt = task_description + \"\\n\\n\"\n",
    "    # Add each example in a consistent format\n",
    "    for example_input, example_output in examples:\n",
    "        prompt += f\"Input: {example_input}\\nOutput: {example_output}\\n\\n\"\n",
    "    # Add the actual input to be processed\n",
    "    prompt += f\"Input: {input_text}\\nOutput:\"\n",
    "    # Call the LLM function with the example-rich prompt\n",
    "    return llm_function(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock LLM function for demonstration purposes\n",
    "# Simulates how a real LLM would respond to different prompts\n",
    "def mock_llm(prompt):\n",
    "    # Check if the prompt is about sentiment analysis\n",
    "    if \"sentiment\" in prompt.lower():\n",
    "        # Simple keyword-based sentiment detection\n",
    "        if \"happy\" in prompt.lower() or \"good\" in prompt.lower():\n",
    "            return \"Positive\"\n",
    "        elif \"sad\" in prompt.lower() or \"bad\" in prompt.lower():\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "    else:\n",
    "        # Default response for other types of prompts\n",
    "        return \"This is a generated response based on the prompt.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage demonstrating different prompting techniques\n",
    "task = \"Classify the sentiment of the following text as Positive, Negative, or Neutral.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot prompting example\n",
    "input_text = \"I love this product!\"\n",
    "zero_shot_result = zero_shot_prompting(mock_llm, task, input_text)\n",
    "print(f\"Zero-shot result: {zero_shot_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting example with training examples\n",
    "examples = [\n",
    "    (\"This is great!\", \"Positive\"),    # Example of positive sentiment\n",
    "    (\"I hate this.\", \"Negative\")       # Example of negative sentiment\n",
    "]\n",
    "few_shot_result = few_shot_prompting(mock_llm, examples, task, input_text)\n",
    "print(f\"Few-shot result: {few_shot_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
