{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module06_text_processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunking.py\n",
    "chunking.py\n",
    "Explanation of Chunking\n",
    "Chunking is the process of breaking down large documents into smaller, manageable pieces.\n",
    "This is essential for RAG systems as LLMs have token limits and work better with focused content.\n",
    "Proper chunking balances context preservation with retrieval efficiency.\n",
    "LangChain text splitting - popular library for text processing in RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_chunking_example():\n",
    "    \"\"\"\n",
    "    Example using LangChain for text chunking.\n",
    "    LangChain provides various text splitters optimized for different use cases.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "        # Sample text about RAG to demonstrate chunking\n",
    "        text = \"\"\"\n",
    "        Retrieval Augmented Generation (RAG) is a technique that combines the power of retrieval systems\n",
    "        with generative AI to provide more accurate and contextually relevant responses. It works by\n",
    "        first retrieving relevant information from a knowledge base, then using that information to\n",
    "        guide the generation of responses by a large language model.\n",
    "\n",
    "        The process typically involves:\n",
    "        1. Converting documents to embeddings\n",
    "        2. Storing embeddings in a vector database\n",
    "        3. For a user query, finding similar documents\n",
    "        4. Providing retrieved documents as context to the LLM\n",
    "\n",
    "        This approach helps overcome some limitations of LLMs, such as outdated knowledge and lack of specific domain expertise.\n",
    "        \"\"\"\n",
    "\n",
    "        # Character-based splitting with paragraph separators\n",
    "        # Splits on double newlines first, then falls back to chunk_size\n",
    "        char_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\\n\",      # Primary separator (paragraph breaks)\n",
    "            chunk_size=200,       # Maximum characters per chunk\n",
    "            chunk_overlap=50      # Characters to overlap between chunks\n",
    "        )\n",
    "        # Split the text into chunks\n",
    "        char_chunks = char_splitter.split_text(text)\n",
    "\n",
    "        # Display character-based splitting results\n",
    "        print(\"LangChain Character Splitting:\")\n",
    "        for i, chunk in enumerate(char_chunks, 1):\n",
    "            print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "            print(f\"Content: {chunk[:100]}...\")\n",
    "            print()\n",
    "\n",
    "        # Recursive character splitting - more sophisticated approach\n",
    "        # Tries multiple separators in order: paragraphs, sentences, words, characters\n",
    "        recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Hierarchy of separators to try\n",
    "            chunk_size=150,     # Smaller chunks for this example\n",
    "            chunk_overlap=30    # Smaller overlap\n",
    "        )\n",
    "        # Split using recursive approach\n",
    "        recursive_chunks = recursive_splitter.split_text(text)\n",
    "\n",
    "        # Display recursive splitting results\n",
    "        print(\"LangChain Recursive Splitting:\")\n",
    "        for i, chunk in enumerate(recursive_chunks, 1):\n",
    "            print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "            print(f\"Content: {chunk[:80]}...\")\n",
    "            print()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"LangChain not installed. Install with: pip install langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_chunking_example():\n",
    "    \"\"\"\n",
    "    Example using spaCy for text chunking based on linguistic features.\n",
    "    spaCy provides advanced NLP capabilities for understanding text structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import spacy\n",
    "\n",
    "        # Load English language model (requires download)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # Short sample text for demonstration\n",
    "        text = \"Retrieval Augmented Generation combines retrieval and generation. It uses external knowledge to improve LLM responses.\"\n",
    "\n",
    "        # Process text through spaCy pipeline to get linguistic analysis\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Sentence-based chunking using spaCy's sentence segmentation\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        print(\"spaCy Sentence Chunking:\")\n",
    "        for i, sent in enumerate(sentences, 1):\n",
    "            print(f\"Sentence {i}: {sent}\")\n",
    "\n",
    "        # Extract noun phrases as meaningful chunks\n",
    "        # Noun phrases represent key concepts and entities\n",
    "        noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "        print(\"\\nNoun Phrases:\")\n",
    "        for phrase in noun_phrases:\n",
    "            print(f\"- {phrase}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"spaCy not installed. Install with: pip install spacy\")\n",
    "        print(\"Also need to download model: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_chunking(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Simple character-based chunking with overlap.\n",
    "    This demonstrates the basic principles of text chunking without external libraries.\n",
    "    \"\"\"\n",
    "    # List to store the resulting chunks\n",
    "    chunks = []\n",
    "    # Starting position for chunking\n",
    "    start = 0\n",
    "\n",
    "    # Continue chunking until we've covered the entire text\n",
    "    while start < len(text):\n",
    "        # Calculate end position for this chunk\n",
    "        end = start + chunk_size\n",
    "        # Extract the chunk\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        # If not at the end of text, try to break at sentence boundaries\n",
    "        if end < len(text):\n",
    "            # Look for sentence endings in the last 100 characters of chunk\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_question = chunk.rfind('?')\n",
    "            last_exclamation = chunk.rfind('!')\n",
    "\n",
    "            # Find the latest sentence ending\n",
    "            break_point = max(last_period, last_question, last_exclamation)\n",
    "            # If break point is reasonable (not too early in chunk), use it\n",
    "            if break_point > chunk_size - 100:  # If break point is reasonable\n",
    "                end = start + break_point + 1\n",
    "                chunk = text[start:end]\n",
    "\n",
    "        # Add the chunk to results (strip whitespace)\n",
    "        chunks.append(chunk.strip())\n",
    "        # Move start position with overlap for next chunk\n",
    "        start = end - overlap\n",
    "\n",
    "    # Return the list of text chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of custom chunking\n",
    "sample_text = \"\"\"\n",
    "RAG systems work by retrieving relevant information from a knowledge base before generating responses. This retrieval step helps ground the generation in factual information, reducing hallucinations and improving accuracy. The retrieved documents are typically provided as context to the language model, which then generates a response based on both the query and the retrieved information.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom Chunking:\")\n",
    "chunks = custom_chunking(sample_text, chunk_size=150, overlap=30)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars): {chunk[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of different chunking strategies:\n",
    "- Fixed size: Simple but may break sentences - good for basic use cases\n",
    "- Sentence-based: Respects linguistic boundaries - preserves meaning\n",
    "- Paragraph-based: Preserves document structure - maintains context\n",
    "- Semantic: Based on topic changes (more advanced) - requires ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practices for effective chunking in RAG systems:\n",
    "- Consider LLM token limits (typically 4096-8192 tokens)\n",
    "- Balance chunk size vs. retrieval precision (smaller chunks = more precise retrieval)\n",
    "- Include overlap to maintain context across chunk boundaries\n",
    "- Experiment with different strategies for your specific use case and data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
