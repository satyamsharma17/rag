{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module05_indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision_problem.py\n",
    "precision_problem.py\n",
    "Explanation of Precision Problem\n",
    "In vector databases and similarity search, precision refers to the accuracy of retrieval.\n",
    "Approximate indexing methods trade some precision for speed.\n",
    "The precision problem occurs when the approximate results differ significantly from exact results.\n",
    "This is a fundamental trade-off in large-scale similarity search.\n",
    "Import required libraries for precision analysis\n",
    "numpy for numerical operations and random number generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# cosine_similarity for measuring vector similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact k-nearest neighbor search using brute force approach\n",
    "def exact_search(query, vectors, k=5):\n",
    "    \"\"\"\n",
    "    Exact k-nearest neighbor search using brute force.\n",
    "    This calculates similarity to ALL vectors and returns the top-k most similar.\n",
    "    Time complexity: O(n) where n is the number of vectors.\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarity between query and all vectors\n",
    "    similarities = cosine_similarity([query], vectors)[0]\n",
    "    # Get indices of top-k most similar vectors (highest similarity first)\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    # Return list of (index, similarity) tuples\n",
    "    return [(idx, similarities[idx]) for idx in top_k_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate approximate search with controlled error to demonstrate precision issues\n",
    "def approximate_search(query, vectors, k=5, error_rate=0.1):\n",
    "    \"\"\"\n",
    "    Simulate approximate search with some error.\n",
    "    This represents what happens with indexing methods like HNSW, IVF, or LSH.\n",
    "    They don't check all vectors, leading to potential misses or ranking errors.\n",
    "    \"\"\"\n",
    "    # Get more candidates than needed (simulating broader search in indexing)\n",
    "    exact_results = exact_search(query, vectors, k=k*2)  # Get more candidates\n",
    "\n",
    "    # Introduce approximation error by adding noise to similarities\n",
    "    approximate_results = []\n",
    "    for idx, sim in exact_results[:k]:\n",
    "        # Add Gaussian noise to simulate indexing approximation errors\n",
    "        noise = np.random.normal(0, error_rate)\n",
    "        # Clamp result to [0,1] range (cosine similarity bounds)\n",
    "        approx_sim = max(0, min(1, sim + noise))  # Keep in [0,1]\n",
    "        approximate_results.append((idx, approx_sim))\n",
    "\n",
    "    # Re-sort after introducing approximation errors\n",
    "    approximate_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Return top-k approximate results\n",
    "    return approximate_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world precision characteristics of different indexing methods:\n",
    "- HNSW: High precision (>0.95) with good speed - best overall trade-off\n",
    "- IVF: Good precision with coarse quantization - scales well to large datasets\n",
    "- LSH: Lower precision but very fast - good for high-throughput scenarios\n",
    "- Trade-offs depend on use case (real-time search vs. accuracy-critical applications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies for mitigating precision problems in production:\n",
    "- Use higher-quality indexing parameters (more clusters, larger hash tables)\n",
    "- Implement re-ranking: get more candidates from index, then re-rank exactly\n",
    "- Hybrid approaches: combine multiple indexing methods for better coverage\n",
    "- Regular precision monitoring and tuning based on your specific data distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing_algorithms.py\n",
    "indexing_algorithms.py\n",
    "Explanation of Indexing Algorithms\n",
    "Hierarchical Navigable Small World (HNSW) is a graph-based indexing algorithm.\n",
    "It creates a multi-layer graph structure for efficient nearest neighbor search.\n",
    "HNSW is widely used in production vector databases for its speed-accuracy balance.\n",
    "Import required libraries for HNSW demonstration\n",
    "numpy for vector operations and array handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# cosine_similarity for measuring vector similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# heapq for priority queue operations (though not used in this simplified version)\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified HNSW implementation for educational purposes\n",
    "# Real HNSW is much more complex with multiple layers and sophisticated algorithms\n",
    "class SimpleHNSW:\n",
    "    # Initialize HNSW structure with maximum connections per node\n",
    "    def __init__(self, max_connections=16):\n",
    "        # List to store all vectors in the index\n",
    "        self.vectors = []\n",
    "        # Graph structure: each index contains list of connected neighbor indices\n",
    "        self.graph = []  # List of neighbor lists\n",
    "        # Maximum number of connections per node (controls graph density)\n",
    "        self.max_connections = max_connections\n",
    "\n",
    "    # Add a vector to the HNSW structure\n",
    "    def add_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Add a vector to the HNSW structure.\n",
    "        In real HNSW, this involves probabilistic insertion into multiple layers.\n",
    "        \"\"\"\n",
    "        # Convert input to numpy array for consistent handling\n",
    "        vector = np.array(vector)\n",
    "        # Add vector to the storage\n",
    "        self.vectors.append(vector)\n",
    "        # Get the index of the newly added vector\n",
    "        idx = len(self.vectors) - 1\n",
    "        # Initialize empty neighbor list for this vector\n",
    "        self.graph.append([])\n",
    "\n",
    "        # Only create connections if we have more than one vector\n",
    "        if len(self.vectors) > 1:\n",
    "            # Find nearest neighbors to connect with (simplified approach)\n",
    "            neighbors = self._find_nearest_neighbors(vector, k=self.max_connections)\n",
    "            # Store neighbors for this vector\n",
    "            self.graph[idx] = neighbors\n",
    "\n",
    "            # Add bidirectional connections (undirected graph)\n",
    "            for neighbor_idx in neighbors:\n",
    "                # Only add if not already connected (avoid duplicates)\n",
    "                if idx not in self.graph[neighbor_idx]:\n",
    "                    self.graph[neighbor_idx].append(idx)\n",
    "\n",
    "    # Helper method to find nearest neighbors (simplified brute force for demo)\n",
    "    def _find_nearest_neighbors(self, query, k):\n",
    "        \"\"\"\n",
    "        Simple nearest neighbor search (brute force for demonstration).\n",
    "        Real HNSW uses hierarchical graph traversal for efficiency.\n",
    "        \"\"\"\n",
    "        # Return empty list if no vectors exist yet\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        # Calculate similarity to all existing vectors except the one being added\n",
    "        similarities = cosine_similarity([query], self.vectors[:-1])[0]  # Exclude self\n",
    "        # Get indices of top-k most similar vectors\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        # Convert to list for easier handling\n",
    "        return top_k_indices.tolist()\n",
    "\n",
    "    # Search method to find k nearest neighbors to a query vector\n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search for k nearest neighbors.\n",
    "        In real HNSW, this uses hierarchical navigation for efficiency.\n",
    "        \"\"\"\n",
    "        # Return empty if no vectors in index\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        # Convert query to numpy array\n",
    "        query = np.array(query)\n",
    "        # Calculate similarity to all vectors in the index\n",
    "        similarities = cosine_similarity([query], self.vectors)[0]\n",
    "        # Get top-k most similar vectors\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "        # Build results with detailed information\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'index': idx,                    # Index in the vectors list\n",
    "                'vector': self.vectors[idx],     # The actual vector\n",
    "                'similarity': similarities[idx]  # Similarity score\n",
    "            })\n",
    "\n",
    "        # Return the k most similar vectors\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage demonstrating HNSW indexing\n",
    "# Create HNSW index instance\n",
    "hnsw = SimpleHNSW()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample vectors representing different concepts\n",
    "# These are 3D vectors for simplicity; real embeddings are much higher dimensional\n",
    "vectors = [\n",
    "    [1, 0, 0],      # Vector pointing strongly in x-direction\n",
    "    [0, 1, 0],      # Vector pointing strongly in y-direction\n",
    "    [0, 0, 1],      # Vector pointing strongly in z-direction\n",
    "    [0.5, 0.5, 0],  # Vector in xy-plane\n",
    "    [0, 0.5, 0.5],  # Vector in yz-plane\n",
    "    [0.5, 0, 0.5]   # Vector in xz-plane\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all vectors to the HNSW index\n",
    "for vec in vectors:\n",
    "    hnsw.add_vector(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search with a query vector\n",
    "query = [0.6, 0.4, 0]  # Query vector similar to [0.5, 0.5, 0]\n",
    "results = hnsw.search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display search results\n",
    "print(\"HNSW Search Results:\")\n",
    "for result in results:\n",
    "    print(f\"Similarity: {result['similarity']:.3f}, Vector: {result['vector']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key characteristics of HNSW algorithm:\n",
    "- Multi-layer graph structure: Different layers for coarse and fine search\n",
    "- Hierarchical navigation: Start from top layer, drill down to find neighbors\n",
    "- Probabilistic construction: Random decisions for graph connectivity\n",
    "- Excellent search quality vs. speed trade-off: Fast and accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important indexing approaches used in vector databases:\n",
    "- IVF (Inverted File): Partition vector space into clusters, search within relevant clusters\n",
    "- LSH (Locality Sensitive Hashing): Hash similar vectors to same buckets for quick lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing_approaches.py\n",
    "indexing_approaches.py\n",
    "Explanation of Indexing Approaches\n",
    "Different strategies for organizing and searching high-dimensional vector data.\n",
    "Each approach has different trade-offs between speed, accuracy, and memory usage.\n",
    "Import required libraries for indexing demonstrations\n",
    "numpy for numerical operations and vector handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# KMeans for clustering in IVF implementation\n",
    "from sklearn.cluster import KMeans\n",
    "# cosine_similarity for measuring vector similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted File (IVF) - Partition-based approach\n",
    "# IVF divides vector space into clusters and searches only relevant clusters\n",
    "class SimpleIVF:\n",
    "    # Initialize IVF with number of clusters to create\n",
    "    def __init__(self, n_clusters=10):\n",
    "        # Number of clusters (partitions) to divide the space into\n",
    "        self.n_clusters = n_clusters\n",
    "        # K-means model for clustering (initialized during build_index)\n",
    "        self.kmeans = None\n",
    "        # List of lists: each sublist contains vector indices belonging to that cluster\n",
    "        self.clusters = [[] for _ in range(n_clusters)]\n",
    "        # Storage for all vectors in the index\n",
    "        self.vectors = []\n",
    "\n",
    "    # Build the IVF index by clustering all vectors\n",
    "    def build_index(self, vectors):\n",
    "        \"\"\"\n",
    "        Build IVF index by clustering vectors.\n",
    "        This creates partitions that group similar vectors together.\n",
    "        \"\"\"\n",
    "        # Convert all vectors to numpy arrays for consistent handling\n",
    "        self.vectors = [np.array(v) for v in vectors]\n",
    "        # Convert to numpy array for sklearn compatibility\n",
    "        vectors_array = np.array(vectors)\n",
    "\n",
    "        # Use K-means clustering to partition the vector space\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
    "        # Fit K-means and get cluster assignments for each vector\n",
    "        cluster_labels = self.kmeans.fit_predict(vectors_array)\n",
    "\n",
    "        # Assign each vector to its corresponding cluster\n",
    "        for idx, label in enumerate(cluster_labels):\n",
    "            self.clusters[label].append(idx)\n",
    "\n",
    "    # Search using IVF approach: find nearest clusters, then search within them\n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search using IVF: query nearest clusters first.\n",
    "        This avoids searching the entire dataset by focusing on relevant partitions.\n",
    "        \"\"\"\n",
    "        # Convert query to numpy array\n",
    "        query = np.array(query)\n",
    "\n",
    "        # Find distances to all cluster centers\n",
    "        cluster_distances = cosine_similarity([query], self.kmeans.cluster_centers_)[0]\n",
    "        # Get indices of the 3 nearest clusters (can be tuned)\n",
    "        nearest_cluster_indices = np.argsort(cluster_distances)[::-1][:3]  # Top 3 clusters\n",
    "\n",
    "        # Collect candidate vectors from the nearest clusters\n",
    "        candidates = []\n",
    "        for cluster_idx in nearest_cluster_indices:\n",
    "            # For each vector in this cluster, calculate similarity to query\n",
    "            for vec_idx in self.clusters[cluster_idx]:\n",
    "                similarity = cosine_similarity([query], [self.vectors[vec_idx]])[0][0]\n",
    "                candidates.append((vec_idx, similarity))\n",
    "\n",
    "        # Sort candidates by similarity (highest first) and return top-k\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locality Sensitive Hashing (LSH) - Hash-based approach\n",
    "# LSH hashes similar vectors to the same buckets for efficient lookup\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSH:\n",
    "    # Initialize LSH with multiple hash tables for better accuracy\n",
    "    def __init__(self, num_hash_tables=5, hash_size=8):\n",
    "        # Number of hash tables (more tables = better accuracy but more memory)\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        # Size of hash values (affects collision probability)\n",
    "        self.hash_size = hash_size\n",
    "        # List of hash tables (dictionaries mapping hash values to vector indices)\n",
    "        self.hash_tables = [{} for _ in range(num_hash_tables)]\n",
    "        # Storage for all vectors\n",
    "        self.vectors = []\n",
    "\n",
    "    # Hash function that converts vector to an integer hash value\n",
    "    def _hash_vector(self, vector, seed):\n",
    "        \"\"\"\n",
    "        Simple hash function for vector (not cryptographically secure).\n",
    "        Uses MD5 hash of vector string representation with seed for different tables.\n",
    "        \"\"\"\n",
    "        # Convert vector to string with fixed precision\n",
    "        vec_str = ','.join(f\"{x:.6f}\" for x in vector)\n",
    "        # Create hash with seed to get different hashes for different tables\n",
    "        hash_obj = hashlib.md5(f\"{vec_str}_{seed}\".encode())\n",
    "        # Convert hex hash to integer and take modulo for fixed-size hash\n",
    "        return int(hash_obj.hexdigest(), 16) % (2 ** self.hash_size)\n",
    "\n",
    "    # Add vector to LSH index by hashing it into multiple tables\n",
    "    def add_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Add vector to LSH index.\n",
    "        Each vector is hashed into multiple hash tables for redundancy.\n",
    "        \"\"\"\n",
    "        # Convert to numpy array\n",
    "        vector = np.array(vector)\n",
    "        # Store the vector\n",
    "        self.vectors.append(vector)\n",
    "        # Get index of newly added vector\n",
    "        idx = len(self.vectors) - 1\n",
    "\n",
    "        # Hash the vector into each hash table\n",
    "        for table_idx in range(self.num_hash_tables):\n",
    "            # Generate hash value for this table\n",
    "            hash_value = self._hash_vector(vector, table_idx)\n",
    "            # Initialize bucket if it doesn't exist\n",
    "            if hash_value not in self.hash_tables[table_idx]:\n",
    "                self.hash_tables[table_idx][hash_value] = []\n",
    "            # Add vector index to the hash bucket\n",
    "            self.hash_tables[table_idx][hash_value].append(idx)\n",
    "\n",
    "    # Search using LSH: find vectors that hash to same buckets as query\n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search using LSH: find vectors in same hash buckets.\n",
    "        This is probabilistic - may miss some similar vectors but is very fast.\n",
    "        \"\"\"\n",
    "        # Convert query to numpy array\n",
    "        query = np.array(query)\n",
    "        # Set to collect candidate vector indices (eliminates duplicates)\n",
    "        candidates = set()\n",
    "\n",
    "        # Query each hash table to find potential similar vectors\n",
    "        for table_idx in range(self.num_hash_tables):\n",
    "            # Get hash value for query in this table\n",
    "            hash_value = self._hash_vector(query, table_idx)\n",
    "            # If hash bucket exists, add all vectors in it to candidates\n",
    "            if hash_value in self.hash_tables[table_idx]:\n",
    "                candidates.update(self.hash_tables[table_idx][hash_value])\n",
    "\n",
    "        # Calculate actual similarities for all candidate vectors\n",
    "        results = []\n",
    "        for idx in candidates:\n",
    "            similarity = cosine_similarity([query], [self.vectors[idx]])[0][0]\n",
    "            results.append((idx, similarity))\n",
    "\n",
    "        # Sort by similarity and return top-k results\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage demonstrating both IVF and LSH approaches\n",
    "vectors = [\n",
    "    [1, 0, 0],      # Vector along x-axis\n",
    "    [0, 1, 0],      # Vector along y-axis\n",
    "    [0, 0, 1],      # Vector along z-axis\n",
    "    [0.5, 0.5, 0],  # Vector in xy-plane\n",
    "    [0, 0.5, 0.5],  # Vector in yz-plane\n",
    "    [0.5, 0, 0.5]   # Vector in xz-plane\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate IVF (Inverted File) indexing\n",
    "ivf = SimpleIVF(n_clusters=3)  # Use 3 clusters for this small dataset\n",
    "ivf.build_index(vectors)\n",
    "query = [0.6, 0.4, 0]  # Query vector similar to [0.5, 0.5, 0]\n",
    "ivf_results = ivf.search(query, k=3)\n",
    "print(\"IVF Results:\")\n",
    "for idx, sim in ivf_results:\n",
    "    print(f\"Index: {idx}, Similarity: {sim:.3f}, Vector: {vectors[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LSH (Locality Sensitive Hashing)\n",
    "lsh = SimpleLSH(num_hash_tables=3, hash_size=4)  # 3 hash tables, 4-bit hashes\n",
    "for vec in vectors:\n",
    "    lsh.add_vector(vec)\n",
    "lsh_results = lsh.search(query, k=3)\n",
    "print(\"\\nLSH Results:\")\n",
    "for idx, sim in lsh_results:\n",
    "    print(f\"Index: {idx}, Similarity: {sim:.3f}, Vector: {vectors[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key differences between the approaches:\n",
    "IVF: Fast search by limiting to relevant partitions (clusters), more deterministic\n",
    "LSH: Probabilistic approach, may miss some similar items but very fast and memory efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing.py\n",
    "indexing.py\n",
    "Explanation of Indexing\n",
    "Indexing in vector databases refers to data structures that enable fast similarity search.\n",
    "Without indexing, searching would require comparing the query to every vector (brute force).\n",
    "Indexing algorithms organize vectors to reduce search time from O(n) to O(log n) or better.\n",
    "Import required libraries for indexing demonstration\n",
    "KDTree provides exact nearest neighbor search for low-dimensional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "# numpy for numerical operations and random vector generation\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample vectors to demonstrate indexing\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "# Create 100 random 2D vectors (points in 2D space)\n",
    "vectors = np.random.rand(100, 2)  # 100 vectors in 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KD-Tree index from the vectors\n",
    "# KD-Tree partitions space into regions for efficient nearest neighbor search\n",
    "kdtree = KDTree(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query point for nearest neighbor search\n",
    "# This represents a query vector we want to find similar vectors for\n",
    "query = np.array([[0.5, 0.5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find k nearest neighbors using the KD-Tree index\n",
    "# query() returns distances and indices of the k closest vectors\n",
    "distances, indices = kdtree.query(query, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the indexing results\n",
    "print(\"KD-Tree Indexing Example:\")\n",
    "print(f\"Query point: {query[0]}\")\n",
    "print(\"Nearest neighbors:\")\n",
    "# Iterate through the results and display each neighbor\n",
    "for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "    print(f\"{i+1}. Distance: {dist:.3f}, Vector: {vectors[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of indexing algorithms used in vector databases:\n",
    "- Exact indexing: KD-Tree, Ball-Tree (work well for low dimensions like 2D-20D)\n",
    "- Approximate indexing: HNSW, IVF, LSH (optimized for high dimensions like 384D-1536D)\n",
    "- Trade-off: Approximate methods sacrifice some accuracy for massive speed improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note about dimensionality:\n",
    "In high-dimensional spaces (>100D), traditional tree structures become ineffective\n",
    "This is known as the \"curse of dimensionality\" - distances become meaningless\n",
    "Approximate methods like HNSW (Hierarchical Navigable Small World) are used instead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key benefits of indexing for RAG systems:\n",
    "- Faster search: Reduces time complexity from O(n) linear scan to O(log n) or better\n",
    "- Scalability: Enables handling millions/billions of vectors efficiently\n",
    "- Memory efficiency: Uses compressed representations and approximate structures\n",
    "- Real-time performance: Essential for production RAG systems requiring low latency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
